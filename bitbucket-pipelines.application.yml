image: python:3.11

# enable Docker for your repository
options:
  docker: true

definitions:
  services:
    postgres:
      image: postgres
      variables:
        POSTGRES_DB: "pipelines"
        POSTGRES_USER: "test_user"
        POSTGRES_PASSWORD: "test_user_password"

  steps:
    - step: &django-tests
        name: Run Django Tests
        caches:
          - pip
        script:
          - cd $BITBUCKET_CLONE_DIR/polar_labs
          # match poetry version with the one you're using for your project
          - pip install --no-cache-dir poetry==1.8.2
          - poetry config virtualenvs.in-project true
          - poetry install --no-root --compile
          - export DATABASE_URL=postgresql://test_user:test_user_password@localhost:5432/pipelines
          - poetry run python manage.py ensure_migrations
          - poetry run python manage.py ensure_tests_run
          - poetry run python manage.py test
        services:
          - postgres

    # You must set the env var API_TEST_URL in Bitbucket settings for this to work as expected
    # So this is currently a bit of a hack solution:
    # We use the same set of tests from the Django LiveServerTestCase tests we already have in our codebase
    # that run against the automagic "ghost server" Django boots when you use that class. However, as a quick and
    # dirty way to test a live server, we swap the url from the one Django provides to the one set in the env var
    # API_TEST_URL if it is present. However, Django doesn't really know that we did that and will still 1) try to
    # run unit tests since we haven't implemented a way to run them separately yet, and 2) it will still try to create
    # a test database. So as a stop-gap solution, we are giving it the location of the Bitbucket Pipelines Postgres so
    # it can make a test database still and be happy, even though it's running it's live server tests against the
    # staging or prod API urls.
    - step: &live-django-tests
        name: Run Django Tests Against Live Server
        caches:
          - pip
        script:
          - source set_env.sh
          - cd $BITBUCKET_CLONE_DIR/polar_labs
          # match poetry version with the one you're using for your project
          - pip install --no-cache-dir poetry==1.8.2
          - poetry config virtualenvs.in-project true
          - poetry install --no-root --compile
          - poetry self add 'poethepoet[poetry_plugin]'
          - echo "$API_TEST_URL"
          - poetry exec run_integ_tests

        services:
          - postgres

    - step: &build-docker
        name: Upload image to Digital Ocean
        image: alpine/doctl:1.27.13 # comes with doctl, kubectl, and helm
        script:
          - doctl registry login --expiry-seconds 3600
          - export IMAGE_NAME=$IMAGE_TAG_NAME:$BITBUCKET_COMMIT
          - docker build -t $IMAGE_NAME $BITBUCKET_CLONE_DIR
          - docker push $IMAGE_NAME
        services:
          - docker

    - step: &helm-deploy
        name: Deploy to Kubernetes
        image: alpine/doctl:1.27.13
        script:
          - source set_env.sh
          - export IMAGE_NAME=$IMAGE_TAG_NAME:$BITBUCKET_COMMIT # this is deterministic, so we can just set it again here
          - export DATABASE_URL=postgresql://$DB_USER:$DB_PASS@$DB_HOST/$DB_NAME?$DB_PARAMS
          - doctl kubernetes cluster kubeconfig save $KUBE_CLUSTER_ID
          - apk add gettext moreutils
          - helm dependency update ./api_chart
          - envsubst < ./api_chart/values.yaml | sponge ./api_chart/values.yaml
          - cat ./api_chart/values.yaml
          # - helm install $APP_NAME-$ENV_NAME ./api_chart --dry-run
          - helm upgrade $APP_NAME-$ENV_NAME ./api_chart --debug --install --force --timeout=120s --wait

    - step: &create-preview-db
        name: Create Preview DB
        image: alpine/doctl:1.27.13 # comes with doctl, kubectl, and helm
        script:
          - source set_env.sh
          # a little hack to capture the output of the create command. We don't want to fail the pipeline
          # if the db already exists, so we capture it, output it, check for the db already exists error.
          # If we find that error, just exit successfully so we continue the pipeline. If we find Error in
          # the output AFTER that, it is unexpected and we fail the pipeline.
          - export DB_CREATE_OUTPUT=$(doctl databases db create $DB_CLUSTER_ID $DB_NAME 2>&1 > /dev/null)
          - echo "$DB_CREATE_OUTPUT"
          - echo $DB_CREATE_OUTPUT | grep -q "database name is not available" && exit 0
          - echo $DB_CREATE_OUTPUT | grep -q "Error" && exit 1

pipelines:
  branches:
    main:
      - step: *django-tests

      # deploy to staging
      - stage:
          name: Deploy to Staging
          deployment: staging
          steps:
            - step:
                name: Setup Environment
                script:
                  # set deployment env vars
                  - echo "export ENV_NAME=staging" >> set_env.sh
                artifacts:
                  - set_env.sh

            - step: *build-docker
            - step: *helm-deploy
            - step: *live-django-tests

      - stage:
          name: Deploy to Production
          deployment: production
          trigger: manual
          steps:
            - step:
                name: Setup Environment
                image: getsentry/sentry-cli
                script:
                  # set deployment env vars
                  - echo "export ENV_NAME=production" >> set_env.sh
                  # configure sentry release
                  # SENTRY_AUTH_TOKEN, SENTRY_ORG, and SENTRY_PROJECT set in repository vars
                  # propose-version assumes you're in a git repository
                  - export SENTRY_RELEASE_NAME=$(sentry-cli releases propose-version)
                  - sentry-cli releases new $SENTRY_RELEASE_NAME
                  - sentry-cli releases set-commits --auto --ignore-missing $SENTRY_RELEASE_NAME
                  # add the env var to our set environment script for later deploy steps
                  - echo "export SENTRY_RELEASE_NAME=$SENTRY_RELEASE_NAME" >> set_env.sh
                artifacts:
                  - set_env.sh

            - step: *build-docker
            - step: *helm-deploy
            - step:
                name: Finalize Release
                image: getsentry/sentry-cli
                script:
                  - source set_env.sh
                  - sentry-cli releases finalize $SENTRY_RELEASE_NAME
            - step: *live-django-tests

  pull-requests:
    "**": # this runs as default for any branch not elsewhere defined
      - step: *django-tests
      - stage:
          name: Deploy Preview Environment
          deployment: preview
          trigger: manual
          steps:
            - step:
                name: Setup Environment
                script:
                  - echo "export ENV_NAME=preview-$BITBUCKET_PR_ID" >> set_env.sh
                  - echo "export DB_NAME=preview-$BITBUCKET_PR_ID" >> set_env.sh # we only set this for preview env cause it's dynamic, permanent envs are in bitbucket settings
                  - echo "export API_TEST_URL=https://preview-$BITBUCKET_PR_ID-api.$BASE_URL" >> set_env.sh
                artifacts:
                  - set_env.sh

            - step: *build-docker
            - step: *create-preview-db
            - step: *helm-deploy
            - step: *live-django-tests

  custom:
    close-preview-environment: #name of this pipeline
      - variables:
          - name: BITBUCKET_PR_ID
      - step:
          name: Delete Preview Environment
          image: alpine/doctl:1.27.13
          script:
            - export ENV_NAME=preview-$BITBUCKET_PR_ID
            - doctl kubernetes cluster kubeconfig save $KUBE_CLUSTER_ID
            - helm uninstall $APP_NAME-$ENV_NAME
            - doctl databases db delete $DB_CLUSTER_ID preview-$BITBUCKET_PR_ID --force # make sure this matches the env creation of $DB_NAME above
      - step:
          name: Delete Preview Environment Static Files
          image: amazon/aws-cli
          script:
            # access key, secret key, and endpoint url env vars are set in BB repository variables
            - export ENV_NAME=preview-$BITBUCKET_PR_ID
            - export AWS_ACCESS_KEY_ID=$AWS_S3_ACCESS_KEY_ID
            - export AWS_SECRET_ACCESS_KEY=$AWS_S3_SECRET_ACCESS_KEY
            # Look for the Django setting AWS_LOCATION to check for the name of the preview folder
            - export S3_LOCATION=s3://$AWS_STORAGE_BUCKET_NAME/$STORAGES_PREFIX-$ENV_NAME
            - aws --endpoint-url $AWS_S3_ENDPOINT_URL s3 rm --recursive $S3_LOCATION
